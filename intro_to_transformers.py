#%% -*- coding: utf-8 -*-
"""Intro to transformers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bSQwJ8HiyJh_v_TnBCJl_FoI7WWhzqI4

**Why should we care about transformers?**

Transformers are proving to be the most influential and powerful models. They dominate every domain now, natural language processing, computer vision, time series and even some aspects of tabular data now. 

As a data scientist understanding transformers going forward is going to be an extremely powerful skill. In this notebook specifically we will look at the natural language processing side of transformers, but don't take for granted that they can be extremely powerful on many problems. 

We can look on https://paperswithcode.com/sota/ and see all of the various domains and their benchmark tasks and see that for almost all of them transformers are surpassing CNNs, RNNs etc.


On the documentation page for the BERT model we can find a link to the original BERT paper if you would like to dive in in more detail.

https://huggingface.co/docs/transformers/model_doc/bert 

https://arxiv.org/abs/1810.04805

For now I just want us to look at a few specific parameters and discuss them. 
https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertConfig

Transformers have become so standard that there is a standard convention for sizing them and just tweaking these can yield us the majority of transformer architectures out there. We don't need to custom write our own implementations from scratch

Looking at a few of the important ones we can see:


*   Vocab size - the number of tokens that the model actually understands
*   Hidden size - this is the dimensionality that we use to represent each token, we can kind of view this as the width of the network
*   Number of hidden layers - The number of transformer blocks that are stacked together. We can view this as the depth of the network

There are many other implementational details but those broad ones will be very important and common across pretty much all architectures.  

BERT came originally in two sizes, base and large. In the original paper base BERT had a vocab size of 30522, hidden size of 768, and hidden layer count of 12. This yields a model with 100+ million parameters. The large variant is over 300+ million parameters. These were once considered ludicrously large but now we have scaled even beyond a trillion parameters.  This base implementation has become extremely common. Lots of derivative works have borrowed this same size of model and then tried to tweak various things around it. What flavor of attention, the data that is used for training, the method of pretraining, etc.

---
<img src = "https://i.imgur.com/5c0T3hK.png">

One of the most important things to understand as a data scientist/machine learning practitioner are the inputs and outputs so we will walk through what actually goes into the model and what comes out of the model. 

On the input side, one of our problems is that when operating on text we cant directly do any math on strings so we need to find some numerical representation for them. In the past people have done work to try to find meaninful vectors that represent each word individually and they build up a massive file represents the whole vocabulary. This is ok but it misses big when a word occurs that has no vector available. Language has a very long tail and it is difficult to cover all possible words and mispelling and random combination of characters and it also doesnt take advantage of the common elements in language like words ending in "logy" or words that begin with "chem". Many roots have similar meanings and if you operate at the word level it is hard to represent these commonalities. 

Instead of doing this the language model transformers will operate on tokens instead of words. They will break the words up into chunks. There are a few common options here, byte pair encoding, wordpiece, etc. They all functionally do the same thing though of splitting a word into its constituent pieces. 

So when we looked at the BERT vocab size of 30522, that doesn't mean it only knows 30522 words, that means it knows that many tokens and that many tokens can recreate basically any combination of characters possible and it will have some understanding. 

The tokenizer will split the word into pieces and then gives it an ID. This ID then can be mapped to the correct embedding/vector that represents that token. At that point we have succesfully gone from strings to numbers that mean something for the model. Lets try that out
"""

#%%
import numpy as np
import torch
# Transformers installation
# ! pip install transformers datasets
# To install from source instead of the last release, comment the command above and uncomment the following one.
# ! pip install git+https://github.com/huggingface/transformers.git

from datasets import load_dataset

dataset = load_dataset("yelp_review_full")
dataset["train"][100]

#%%
"""The transformers package has functions for all of the major models tokenization so we can just borrow these existing implementations. In this one we can load in the tokenizer for bert base. There are a few important parameters here. The model is only capable of handling up to 512 tokens at a time so we might need to cut some text off or break it into multiple parts if the text is longer than that. Additionally, since we typically want to train models with batches of data at a time we want them all to be sized the same so we will pad the text to the maximum length of the model (512 tokens) so that it is one big batched tensor we can pass to the model instead of individual vectors or a ragged tensor. """

dataset["train"] = dataset["train"].shuffle(seed=42).select(range(1000))
dataset["test"] = dataset["test"].shuffle(seed=42).select(range(1000))

dataset["train"][0]

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")


def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True, max_length = 128)


tokenized_datasets = dataset.map(tokenize_function, batched=True)

"""If you like, you can create a smaller subset of the full dataset to fine-tune on to reduce the time it takes:"""

small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))

small_train_dataset[0]["text"]

small_train_dataset[0]["label"]

torch.tensor(small_train_dataset[0]["input_ids"])

"""<a id='trainer'></a>"""

np.array(tokenizer.convert_ids_to_tokens(small_train_dataset[0]["input_ids"]))

"""## Train with PyTorch Trainer

ðŸ¤— Transformers provides a [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) class optimized for training ðŸ¤— Transformers models, making it easier to start training without manually writing your own training loop. The [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) API supports a wide range of training options and features such as logging, gradient accumulation, and mixed precision.

Start by loading your model and specify the number of expected labels. From the Yelp Review [dataset card](https://huggingface.co/datasets/yelp_review_full#data-fields), you know there are five labels:
"""

from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=5)

"""<Tip>

You will see a warning about some of the pretrained weights not being used and some weights being randomly
initialized. Don't worry, this is completely normal! The pretrained head of the BERT model is discarded, and replaced with a randomly initialized classification head. You will fine-tune this new model head on your sequence classification task, transferring the knowledge of the pretrained model to it.

</Tip>

### Training hyperparameters

Next, create a [TrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments) class which contains all the hyperparameters you can tune as well as flags for activating different training options. For this tutorial you can start with the default training [hyperparameters](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments), but feel free to experiment with these to find your optimal settings.

Specify where to save the checkpoints from your training:
"""

from transformers import TrainingArguments

training_args = TrainingArguments(output_dir="test_trainer")

"""### Evaluate"""

!pip install evaluate

"""[Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) does not automatically evaluate model performance during training. You'll need to pass [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) a function to compute and report metrics. The [ðŸ¤— Evaluate](https://huggingface.co/docs/evaluate/index) library provides a simple [`accuracy`](https://huggingface.co/spaces/evaluate-metric/accuracy) function you can load with the [evaluate.load](https://huggingface.co/docs/evaluate/main/en/package_reference/loading_methods#evaluate.load) (see this [quicktour](https://huggingface.co/docs/evaluate/a_quick_tour) for more information) function:"""

import numpy as np
import evaluate

metric = evaluate.load("accuracy")

"""Call `compute` on `metric` to calculate the accuracy of your predictions. Before passing your predictions to `compute`, you need to convert the predictions to logits (remember all ðŸ¤— Transformers models return logits):"""

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

"""If you'd like to monitor your evaluation metrics during fine-tuning, specify the `evaluation_strategy` parameter in your training arguments to report the evaluation metric at the end of each epoch:"""

from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(output_dir="test_trainer", evaluation_strategy="epoch")

"""### Trainer

Create a [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) object with your model, training arguments, training and test datasets, and evaluation function:
"""

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
)

"""Then fine-tune your model by calling [train()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train):"""

trainer.train()

"""<a id='pytorch_native'></a>

## Train in native PyTorch
"""

del model
del trainer
torch.cuda.empty_cache()

tokenized_datasets = tokenized_datasets.remove_columns(["text"])
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
tokenized_datasets.set_format("torch")

small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))

"""### DataLoader

Create a `DataLoader` for your training and test datasets so you can iterate over batches of data:
"""

from torch.utils.data import DataLoader

train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=32)
eval_dataloader = DataLoader(small_eval_dataset, batch_size=32)

"""Load your model with the number of expected labels:"""

from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=5)

"""### Optimizer and learning rate scheduler

Create an optimizer and learning rate scheduler to fine-tune the model. Let's use the [`AdamW`](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html) optimizer from PyTorch:
"""

from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)

"""Create the default learning rate scheduler from [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer):"""

from transformers import get_scheduler

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    name="linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps
)

"""Lastly, specify `device` to use a GPU if you have access to one. Otherwise, training on a CPU may take several hours instead of a couple of minutes."""

import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)

"""<Tip>

Get free access to a cloud GPU if you don't have one with a hosted notebook like [Colaboratory](https://colab.research.google.com/) or [SageMaker StudioLab](https://studiolab.sagemaker.aws/).

</Tip>

Great, now you are ready to train! ðŸ¥³

### Training loop

To keep track of your training progress, use the [tqdm](https://tqdm.github.io/) library to add a progress bar over the number of training steps:
"""

from tqdm.auto import tqdm

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

"""### Evaluate

Just like how you added an evaluation function to [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer), you need to do the same when you write your own training loop. But instead of calculating and reporting the metric at the end of each epoch, this time you'll accumulate all the batches with `add_batch` and calculate the metric at the very end.
"""

import evaluate

metric = evaluate.load("accuracy")
model.eval()
for batch in eval_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}
    with torch.no_grad():
        outputs = model(**batch)

    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    metric.add_batch(predictions=predictions, references=batch["labels"])

metric.compute()

"""<a id='additional-resources'></a>

## Additional resources

For more fine-tuning examples, refer to:

- [ðŸ¤— Transformers Examples](https://github.com/huggingface/transformers/tree/main/examples) includes scripts
  to train common NLP tasks in PyTorch and TensorFlow.

- [ðŸ¤— Transformers Notebooks](https://huggingface.co/docs/transformers/main/en/notebooks) contains various notebooks on how to fine-tune a model for specific tasks in PyTorch and TensorFlow.
"""